{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet Success: Analyzing Recipe Ratings with Data Science\n",
    "\n",
    "**Name(s)**: Timothy Kam\n",
    "\n",
    "**Website Link**: https://psionergy.github.io/recipe-analysis-project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import pearsonr\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Introduction\n",
    "The dataset contains two key files:\n",
    "1. **Recipes Dataset (`RAW_recipes.csv`)**:\n",
    "  - Provides recipe details such as preparation time, number of ingredients, and nutritional information.\n",
    "2. **Interactions Dataset (`RAW_interactions.csv`)**:\n",
    "  - Contains user interactions with recipes, including ratings and reviews.\n",
    "\n",
    "# Research Question\n",
    "**How do preparation time and the number of ingredients influence recipe ratings?**\n",
    "\n",
    "# Importance\n",
    "This question can help identify user preferences in recipe creation and determine if simplicity in recipes is associated with higher satisfaction.\n",
    "\n",
    "\n",
    "Step 1: Introduction and Question Identification\n",
    "\n",
    "Dataset Introduction\n",
    "The dataset I am working with is the **Recipes and Ratings** dataset. It contains information about recipes, their preparation times, ingredients, user ratings, and reviews.\n",
    "\n",
    "Central Question\n",
    "How do the number of ingredients and preparation time affect recipe ratings?*\n",
    "\n",
    "Why It Matters\n",
    "Understanding these relationships can help recipe creators optimize their content for better user satisfaction. It also sheds light on user behavior and preferences in cooking.\n",
    "\n",
    "# Relevant Columns\n",
    "- `rating`: User rating of the recipe (target variable).\n",
    "- `n_ingredients`: Number of ingredients in a recipe.\n",
    "- `minutes`: Preparation time for the recipe.\n",
    "\n",
    "# Dataset Overview\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the datasets for analysis, we began by merging the RAW_recipes.csv and RAW_interactions.csv datasets using a left join on the \n",
    "id column from the recipes dataset and the recipe_id column from the interactions dataset. This step ensured that all recipes, regardless \n",
    "of whether they had ratings, were retained in the merged dataset. We then replaced all ratings of 0 with NaN, as a rating of 0 likely \n",
    "indicates missing data rather than an actual evaluation. This avoids skewing the average ratings calculation toward lower values. \n",
    "Finally, we calculated the average rating for each recipe and added this as a new column in the recipes dataset. These steps ensured the \n",
    "data was clean, consistent, and suitable for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "recipes = pd.read_csv('food_data/RAW_recipes.csv')\n",
    "ratings = pd.read_csv('food_data/RAW_interactions.csv')\n",
    "# 1. Left merge recipes and interactions\n",
    "df = pd.merge(recipes, ratings, left_on='id', right_on='recipe_id', how='left')\n",
    "\n",
    "# 2. Replace ratings of 0 with np.nan\n",
    "df['rating'] = df['rating'].replace(0, np.nan)\n",
    "\n",
    "# 3. Calculate average rating per recipe\n",
    "avg_ratings = df.groupby('id')['rating'].mean()\n",
    "\n",
    "# 4. Add average ratings to merged dataframe\n",
    "df['avg_rating'] = df['id'].map(avg_ratings)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dist = df['rating'].value_counts().sort_index()\n",
    "fig1 = px.bar(\n",
    "    x=ratings_dist.index,\n",
    "    y=ratings_dist.values,\n",
    "    labels={'x': 'Rating', 'y': 'Frequency'},\n",
    "    title='Distribution of Ratings'\n",
    ")\n",
    "\n",
    "\n",
    "fig1.update_layout(\n",
    "    title={\n",
    "        'text': 'Distribution of Ratings',\n",
    "        'x': 0.5, \n",
    "        'xanchor': 'center'\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title=\"Rating\",\n",
    "        tickmode='linear',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Frequency\",\n",
    "        tickformat=\",\",\n",
    "    ),\n",
    "    width=800, \n",
    "    height=600, \n",
    "    plot_bgcolor='white'  \n",
    ")\n",
    "\n",
    "fig1.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
    "fig1.update_yaxes(showgrid=True, gridcolor='lightgray')\n",
    "\n",
    "fig1.show()\n",
    "fig1.write_html('assets/ratings_distribution.html', include_plotlyjs='cdn')\n",
    "\n",
    "\n",
    "\n",
    "filtered_recipes = df[df['minutes'] <= 500]\n",
    "\n",
    "fig2 = px.histogram(\n",
    "    filtered_recipes,\n",
    "    x='minutes',\n",
    "    nbins=50,\n",
    "    title='Distribution of Recipe Preparation Times (0-500 Minutes)',\n",
    "    labels={'minutes': 'Preparation Time (Minutes)', 'count': 'Recipe Count'}\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    xaxis_title=\"Preparation Time (Minutes)\",\n",
    "    yaxis_title=\"Number of Recipes\",\n",
    "    xaxis=dict(range=[0, 500])  # Set the range explicitly\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "fig2.write_html('assets/preparation_times.html', include_plotlyjs='cdn')\n",
    "\n",
    "\n",
    "# Plot 3: Distribution of Number of Ingredients\n",
    "fig3 = px.histogram(\n",
    "   df,\n",
    "   x='n_ingredients',\n",
    "   nbins=30,\n",
    "   title='Distribution of Number of Ingredients per Recipe',\n",
    "   labels={'n_ingredients': 'Number of Ingredients', 'count': 'Number of Recipes'}\n",
    ")\n",
    "\n",
    "fig3.update_layout(\n",
    "   xaxis_title=\"Number of Ingredients\",\n",
    "   yaxis_title=\"Number of Recipes\",\n",
    "   width=800,\n",
    "   height=600,\n",
    "   plot_bgcolor='white',\n",
    "   xaxis=dict(range=[0, 30]) \n",
    ")\n",
    "\n",
    "fig3.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
    "fig3.update_yaxes(showgrid=True, gridcolor='lightgray')\n",
    "\n",
    "fig3.show()\n",
    "fig3.write_html('assets/ingredients_distribution.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bivariate Analysis:\n",
    "For the bivariate analysis, we examined the relationships between pairs of columns to identify meaningful associations. First, we analyzed\n",
    "the relationship between preparation time and the number of ingredients using a scatter plot. This plot shows that recipes with more \n",
    "ingredients tend to require more preparation time, though the correlation weakens as preparation time increases. Second, we explored how \n",
    "average ratings have changed over time using a line chart. This visualization reveals that recipe ratings have remained consistently high \n",
    "across years, with only slight fluctuations. These analyses provide insights into how different features of the recipes interact and inform \n",
    "potential hypotheses for further investigation.\n",
    "Bivariate Analysis: Number of Ingredients vs. Preparation Time\n",
    "In addition to examining the scatterplot of preparation time against the number of ingredients, we explored the average preparation time \n",
    "for each number of ingredients. The resulting bar chart highlights a trend where recipes with fewer ingredients generally require less \n",
    "preparation time. However, there are noticeable spikes for recipes with either very few (1-2) or many ingredients (around 25), suggesting \n",
    "potential outliers or specific recipe types driving these averages. This analysis complements the scatterplot by providing aggregate-level \n",
    "insights into how the complexity of a recipe, as measured by its ingredients, correlates with the time required for preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['minutes'] <= 500].copy()\n",
    "\n",
    "# Create scatter plot with improved aesthetics\n",
    "fig4 = px.scatter(\n",
    "    df_filtered,\n",
    "    x='minutes',\n",
    "    y='n_ingredients',\n",
    "    title='Recipe Preparation Time vs. Number of Ingredients',\n",
    "    labels={\n",
    "        'minutes': 'Preparation Time (Minutes)',\n",
    "        'n_ingredients': 'Number of Ingredients'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig4.update_layout(\n",
    "    title={\n",
    "        'text': 'Recipe Preparation Time vs. Number of Ingredients',\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'y': 0.95\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title='Preparation Time (Minutes)',\n",
    "        range=[0, 500],\n",
    "        gridcolor='lightgray',\n",
    "        showgrid=True,\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Number of Ingredients',\n",
    "        range=[0, 30],\n",
    "        gridcolor='lightgray',\n",
    "        showgrid=True,\n",
    "        zeroline=False\n",
    "    ),\n",
    "    width=800,\n",
    "    height=500,\n",
    "    plot_bgcolor='white',\n",
    "    showlegend=False,\n",
    "    margin=dict(l=80, r=20, t=50, b=50) \n",
    ")\n",
    "\n",
    "fig4.update_traces(\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='rgb(99, 110, 250)',\n",
    "        opacity=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "fig4.show()\n",
    "fig4.write_html('assets/prep_time_vs_ingredients.html', include_plotlyjs='cdn')\n",
    "\n",
    "\n",
    "### Bivariate Analysis: Ratings Over Time\n",
    "\n",
    "df['year'] = pd.to_datetime(df['date'], errors='coerce').dt.year\n",
    "ratings_by_year = df.groupby('year')['rating'].mean().reset_index()\n",
    "\n",
    "fig5 = px.line(\n",
    "    ratings_by_year,\n",
    "    x='year',\n",
    "    y='rating',\n",
    "    title='Distribution of Recipe Ratings Over Time',\n",
    "    labels={\n",
    "        'year': 'Year',\n",
    "        'rating': 'Average Rating'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig5.update_layout(\n",
    "    title={\n",
    "        'text': 'Distribution of Recipe Ratings Over Time',\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center'\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title=\"Year\",\n",
    "        tickformat=\"d\", \n",
    "        gridcolor='lightgray',\n",
    "        showgrid=True\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Average Rating\",\n",
    "        range=[4, 5], \n",
    "        gridcolor='lightgray',\n",
    "        showgrid=True\n",
    "    ),\n",
    "    width=800,\n",
    "    height=500,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig5.show()\n",
    "fig5.write_html('assets/ratings_over_time.html', include_plotlyjs='cdn')\n",
    "\n",
    "\n",
    "\n",
    "ingredients_prep_time = df.groupby('n_ingredients')['minutes'].mean().reset_index()\n",
    "ingredients_prep_time.columns = ['Number of Ingredients', 'Average Preparation Time (Minutes)']\n",
    "\n",
    "fig6 = px.bar(\n",
    "    ingredients_prep_time,\n",
    "    x='Number of Ingredients',\n",
    "    y='Average Preparation Time (Minutes)',\n",
    "    title='Average Preparation Time by Number of Ingredients',\n",
    "    labels={\n",
    "        'Number of Ingredients': 'Number of Ingredients',\n",
    "        'Average Preparation Time (Minutes)': 'Avg Preparation Time (Minutes)'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig6.update_layout(\n",
    "    title={\n",
    "        'text': 'Average Preparation Time by Number of Ingredients',\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center'\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title=\"Number of Ingredients\",\n",
    "        gridcolor='lightgray',\n",
    "        showgrid=True\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Average Preparation Time (Minutes)\",\n",
    "        gridcolor='lightgray',\n",
    "        showgrid=True\n",
    "    ),\n",
    "    width=800,\n",
    "    height=500,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig6.show()\n",
    "fig6.write_html('assets/ingredients_vs_prep_time.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting Aggregates\n",
    "We grouped recipes by the number of ingredients and calculated their average preparation time. This grouping reveals that recipes\n",
    "with 5-15 ingredients have relatively stable preparation times, often aligning with user expectations for standard recipes. However, \n",
    "recipes with an unusually high or low number of ingredients exhibit higher preparation times on average, potentially indicating specific \n",
    "recipe types that require additional effort. This insight aligns with our earlier findings about preparation time distribution and provides \n",
    "further evidence that ingredient count significantly impacts user expectations for time investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_time_bins = [0, 30, 60, 90, 120, 150, 180, 240, 300]\n",
    "prep_time_labels = ['0-30', '31-60', '61-90', '91-120', '121-150', '151-180', '181-240', '241-300']\n",
    "\n",
    "df['prep_time_range'] = pd.cut(df['minutes'], bins=prep_time_bins, labels=prep_time_labels, right=False)\n",
    "\n",
    "avg_rating_by_prep_time = (\n",
    "    df.groupby('prep_time_range', observed=False)['rating']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "avg_rating_by_prep_time.columns = ['Preparation Time Range', 'Average Rating']\n",
    "\n",
    "avg_rating_by_prep_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_bins = [0, 5, 10, 15, 20, 30]\n",
    "steps_labels = ['0-5', '6-10', '11-15', '16-20', '21-30']\n",
    "\n",
    "df['step_range'] = pd.cut(df['n_steps'], bins=steps_bins, labels=steps_labels, right=False)\n",
    "\n",
    "avg_rating_by_step_range = (\n",
    "    df.groupby('step_range', observed=False)['rating']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "avg_rating_by_step_range.columns = ['Step Range', 'Average Rating']\n",
    "\n",
    "avg_rating_by_step_range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Create binary missingness indicators\n",
    "df['rating_missing'] = df['rating'].isnull().astype(int)\n",
    "df['review_missing'] = df['review'].isnull().astype(int)\n",
    "df['description_missing'] = df['description'].isnull().astype(int)\n",
    "\n",
    "# 2. Define a function to calculate a test statistic (e.g., Pearson correlation)\n",
    "def calculate_statistic(data, col1, col2):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation between two columns.\n",
    "    \"\"\"\n",
    "    return pearsonr(data[col1], data[col2])[0]\n",
    "\n",
    "# 3. Calculate observed statistics for missingness\n",
    "observed_stat_review = calculate_statistic(df, 'rating_missing', 'review_missing')\n",
    "observed_stat_description = calculate_statistic(df, 'rating_missing', 'description_missing')\n",
    "\n",
    "# 4. Permutation test function\n",
    "def permutation_test(data, col1, col2, n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test for the dependency of missingness.\n",
    "    \"\"\"\n",
    "    original_data = data[col2].copy()\n",
    "    statistics = []\n",
    "    \n",
    "    for _ in range(n_permutations):\n",
    "        np.random.shuffle(original_data.values)\n",
    "        statistic = calculate_statistic(pd.DataFrame({col1: data[col1], col2: original_data}), col1, col2)\n",
    "        statistics.append(statistic)\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "# Run permutation tests\n",
    "perm_stats_review = permutation_test(df, 'rating_missing', 'review_missing')\n",
    "perm_stats_description = permutation_test(df, 'rating_missing', 'description_missing')\n",
    "\n",
    "# Calculate p-values\n",
    "p_value_review = (np.sum(np.abs(perm_stats_review) >= np.abs(observed_stat_review)) + 1) / (len(perm_stats_review) + 1)\n",
    "p_value_description = (np.sum(np.abs(perm_stats_description) >= np.abs(observed_stat_description)) + 1) / (len(perm_stats_description) + 1)\n",
    "\n",
    "print(\"P-value for review missingness:\", p_value_review)\n",
    "print(\"P-value for description missingness:\", p_value_description)\n",
    "\n",
    "# 5. Define a function to create KDE plots\n",
    "def create_kde_plotly(data, col, missing_col, title):\n",
    "    \"\"\"\n",
    "    Create a KDE plot comparing distributions for missing and non-missing data.\n",
    "    \"\"\"\n",
    "    if missing_col not in data.columns:\n",
    "        raise KeyError(f\"Column '{missing_col}' not found in the DataFrame.\")\n",
    "\n",
    "    # Separate the data into missing and not missing\n",
    "    data_missing = data[data[missing_col] == 1][col].dropna()\n",
    "    data_not_missing = data[data[missing_col] == 0][col].dropna()\n",
    "\n",
    "    # Create the KDE plot\n",
    "    fig = ff.create_distplot(\n",
    "        [data_missing, data_not_missing],\n",
    "        group_labels=[f\"{col} (Missing)\", f\"{col} (Not Missing)\"],\n",
    "        show_hist=False,\n",
    "        show_rug=False\n",
    "    )\n",
    "    fig.update_layout(title=title, xaxis_title=col, yaxis_title='Density')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# 6. Create KDE plot for review missingness\n",
    "fig_review = create_kde_plotly(df, 'rating', 'review_missing', 'KDE Plot of Rating by Review Missingness')\n",
    "fig_review.show()\n",
    "\n",
    "# 7. Create KDE plot for description missingness\n",
    "fig_description = create_kde_plotly(df, 'rating', 'description_missing', 'KDE Plot of Rating by Description Missingness')\n",
    "fig_description.show()\n",
    "\n",
    "# 8. Save the plots\n",
    "fig_review.write_html('assets/rating_by_review.html', include_plotlyjs='cdn')\n",
    "fig_description.write_html('assets/rating_by_desc.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 3: Assessment of Missingness\n",
    "NMAR Analysis\n",
    "In this dataset, the review column could be classified as NMAR (Not Missing At Random). This is because whether a user leaves a review \n",
    "or not could depend on personal motivations, such as being highly satisfied or dissatisfied with a recipe. For instance, users who do \n",
    "not feel strongly about the recipe might be less likely to leave a review. Without additional data on user behavior or intent, it is \n",
    "difficult to establish that this missingness depends solely on observable data in the dataset, further supporting the NMAR classification.\n",
    "\n",
    "Missingness Dependency\n",
    "To assess whether the missingness of rating is dependent on other variables, we conducted two separate permutation tests: one examining \n",
    "the relationship between the missingness of rating and review and another examining the relationship between the missingness of rating \n",
    "and description. For these tests, we utilized Pearson Correlation as the test statistic and ran 1000 permutations.\n",
    "\n",
    "Review and Rating\n",
    "Null Hypothesis: The missingness of rating does not depend on the missingness of review.\n",
    "Alternative Hypothesis: The missingness of rating does depend on the missingness of review.\n",
    "Test Statistic: Pearson Correlation\n",
    "Significance Level: 0.05\n",
    "Observed Statistic/P-Value: 0.133 (P-Value)\n",
    "Since the p-value (0.133) is greater than the significance level of 0.05, we fail to reject the null hypothesis. This implies that the \n",
    "    missingness of rating does not appear to depend on the missingness of review.\n",
    "\n",
    "Description and Rating\n",
    "Null Hypothesis: The missingness of rating does not depend on the missingness of description.\n",
    "Alternative Hypothesis: The missingness of rating does depend on the missingness of description.\n",
    "Test Statistic: Pearson Correlation\n",
    "Significance Level: 0.05\n",
    "Observed Statistic/P-Value: 0.517 (P-Value)\n",
    "Similarly, the p-value (0.517) is greater than the significance level of 0.05, leading us to fail to reject the null hypothesis. \n",
    "    This indicates that the missingness of rating does not appear to depend on the missingness of description.\n",
    "\n",
    "Conclusion\n",
    "Based on the results of these permutation tests, there is no significant evidence to suggest that the missingness of rating is \n",
    "    dependent on either review or description. This aligns with our earlier observation that the missingness in review might be \n",
    "    NMAR, while the missingness in other columns could potentially be MAR (Missing At Random) or MCAR (Missing Completely At Random).\n",
    "\n",
    "This explanation aligns with your updated data, results, and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define short and long preparation time categories\n",
    "prep_time_median = df['minutes'].median()\n",
    "short_prep = df[df['minutes'] <= prep_time_median]['rating']\n",
    "long_prep = df[df['minutes'] > prep_time_median]['rating']\n",
    "\n",
    "# Step 2: Perform an independent t-test\n",
    "t_stat, p_value = ttest_ind(short_prep.dropna(), long_prep.dropna(), equal_var=False)\n",
    "\n",
    "# Step 3: Print results\n",
    "print(\"T-statistic:\", t_stat)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Step 4: Descriptive statistics\n",
    "print(\"Descriptive Statistics for Short Preparation Time Recipes:\")\n",
    "print(short_prep.describe())\n",
    "print(\"\\nDescriptive Statistics for Long Preparation Time Recipes:\")\n",
    "print(long_prep.describe())\n",
    "\n",
    "# Step 5: Create a new column for visualization\n",
    "df['prep_time_category'] = df['minutes'].apply(\n",
    "    lambda x: 'Short Prep Time' if x <= prep_time_median else 'Long Prep Time'\n",
    ")\n",
    "\n",
    "# Step 6: Create a DataFrame for grouped bar chart data\n",
    "grouped_data = df.groupby(['prep_time_category', 'rating']).size().reset_index(name='count')\n",
    "\n",
    "# Step 7: Create the grouped bar chart\n",
    "fig7 = px.bar(\n",
    "    grouped_data,\n",
    "    x='rating',\n",
    "    y='count',\n",
    "    color='prep_time_category',\n",
    "    barmode='group',\n",
    "    labels={'rating': 'Rating', 'count': 'Number of Recipes', 'prep_time_category': 'Preparation Time Category'},\n",
    "    title='Ratings Distribution by Preparation Time Category'\n",
    ")\n",
    "\n",
    "# Update layout for aesthetics\n",
    "fig7.update_layout(\n",
    "    title={'x': 0.5},\n",
    "    xaxis=dict(title='Rating'),\n",
    "    yaxis=dict(title='Count'),\n",
    "    legend_title='Preparation Time Category',\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Display and save the chart\n",
    "fig7.show()\n",
    "fig7.write_html('assets/ratings_by_prep_time_grouped.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "As part of our investigation, we aim to understand the potential relationship between a recipe's preparation time and its \n",
    "average rating. This analysis can reveal whether users favor recipes that are quick to prepare or if longer preparation times \n",
    "correlate with higher ratings.\n",
    "\n",
    "**Null Hypothesis**: The preparation time of a recipe does not significantly affect its average rating.\n",
    "\n",
    "**Alternative Hypothesis**: The preparation time of a recipe significantly affects its average rating.\n",
    "\n",
    "**Test Statistic**: The difference in mean ratings between recipes with short preparation times and those with long preparation times.\n",
    "\n",
    "**Significance Level**: 0.05\n",
    "\n",
    "**Observed Statistic/P-Value**: \n",
    "- **T-Statistic**: 11.20122500937791\n",
    "- **P-Value**: 4.0976680888373e-29\n",
    "\n",
    "Based on the results, the p-value is much smaller than the significance level of 0.05, indicating strong evidence to reject the null \n",
    "hypothesis. This suggests that the preparation time of a recipe has a significant impact on its average rating. Specifically, users tend \n",
    "to rate recipes with [shorter/longer] preparation times [higher/lower]. \n",
    "\n",
    "*Visualization*: The box plot below demonstrates the distribution of ratings for recipes categorized as having short and long preparation \n",
    "times, providing further insight into the trend observed in our hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "source": [
    "### Step 5: Framing a Prediction Problem\n",
    "\n",
    "#### **Prediction Problem**\n",
    "I aim to predict the **preparation time** (in minutes) for recipes based on their characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Type of Prediction**\n",
    "This is a **regression problem** since we are predicting a continuous numerical value, which is the preparation time in minutes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Response Variable**\n",
    "The response variable is **`minutes`** (preparation time). I selected this variable because:\n",
    "- It is an objective measure directly related to recipe characteristics.\n",
    "- It has practical applications for users who want to plan their cooking time.\n",
    "- It provides a meaningful target for regression modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Features Available at Prediction Time**\n",
    "I will only use features available **before cooking**. These include:\n",
    "- **`n_ingredients`**: Number of ingredients in the recipe.\n",
    "- **`n_steps`**: Number of steps in the recipe.\n",
    "- **`tags`**: Tags that describe the recipe (e.g., cuisine type or dietary restrictions).\n",
    "- **`description_length`**: The length of the recipe description in words.\n",
    "\n",
    "I will **exclude** features that are unknown before cooking, such as:\n",
    "- **User ratings**\n",
    "- **Reviews**\n",
    "- **User interactions**\n",
    "\n",
    "This ensures the model aligns with real-world scenarios where preparation time must be estimated beforehand.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Evaluation Metric**\n",
    "I will evaluate the model using **Root Mean Squared Error (RMSE)** because:\n",
    "- It is a standard metric for regression problems.\n",
    "- It measures the error in the same units as the target variable (minutes).\n",
    "- It penalizes larger errors more heavily, which is important for time predictions.\n",
    "- It is easy to interpret, as it provides an average measure of how far off the predictions are.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Initial Analysis**\n",
    "From the earlier analysis:\n",
    "- Preparation times range from 0 to 1,051,200 minutes (about 2 years).\n",
    "- Most recipes take between 20–65 minutes (25th to 75th percentile).\n",
    "- The median preparation time is 35 minutes, but there are significant outliers.\n",
    "\n",
    "Correlations with recipe features are weak:\n",
    "- **Number of Ingredients (`n_ingredients`)**: -0.008\n",
    "- **Number of Steps (`n_steps`)**: 0.008\n",
    "\n",
    "This suggests:\n",
    "1. Outliers in the preparation time need to be handled.\n",
    "2. The relationships between preparation time and recipe characteristics may be non-linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter extreme outliers (keep recipes between 5 minutes and 24 hours)\n",
    "df_filtered = df[\n",
    "    (df['minutes'] >= 5) & \n",
    "    (df['minutes'] <= 24*60)\n",
    "].copy()\n",
    "\n",
    "# Select features\n",
    "features = ['n_ingredients', 'n_steps']  # Both quantitative features\n",
    "target = 'minutes'\n",
    "\n",
    "# Prepare features\n",
    "X = df_filtered[features]\n",
    "y = df_filtered[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create pipeline\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_train = baseline_pipeline.predict(X_train)\n",
    "y_pred_test = baseline_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Baseline Model Results:\")\n",
    "print(\"\\nFeature Types:\")\n",
    "print(\"Quantitative features used:\", features)\n",
    "\n",
    "print(\"\\nData Shape After Filtering:\")\n",
    "print(f\"Number of recipes: {len(df_filtered)}\")\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"RMSE: {train_rmse:.4f} minutes\")\n",
    "print(f\"R² Score: {train_r2:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"RMSE: {test_rmse:.4f} minutes\")\n",
    "print(f\"R² Score: {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "for feature, coef in zip(features, baseline_pipeline.named_steps['regressor'].coef_):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Baseline Model\n",
    "\n",
    "\n",
    "Features Used:\n",
    "- n_ingredients (quantitative): Number of ingredients in recipe\n",
    "- n_steps (quantitative): Number of steps in recipe\n",
    "\n",
    "Model Pipeline:\n",
    "1. StandardScaler: Standardizes both features to have mean=0 and variance=1\n",
    "2. LinearRegression: Simple linear model for baseline predictions\n",
    "\n",
    "Results:\n",
    "Test RMSE: 96.90 minutes (~1.6 hours average prediction error)\n",
    "Test R² Score: 0.0285 (explains 2.85% of variance)\n",
    "\n",
    "Feature Coefficients:\n",
    "- n_ingredients: 8.04 (each additional ingredient adds ~8 minutes)\n",
    "- n_steps: 11.40 (each additional step adds ~11 minutes)\n",
    "\n",
    "Model Assessment:\n",
    "While the performance metrics are low, this simple baseline:\n",
    "1. Establishes a minimum performance benchmark\n",
    "2. Shows intuitive relationships (more ingredients/steps = longer time)\n",
    "3. Reveals that preparation time prediction requires more sophisticated features, \n",
    "   which we'll explore in the Final Model (Step 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "### Step 7: Final Model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Use same filtered dataset as baseline\n",
    "df_filtered = df[\n",
    "    (df['minutes'] >= 5) & \n",
    "    (df['minutes'] <= 24*60)\n",
    "].copy()\n",
    "\n",
    "# Engineer new features (without using target variable)\n",
    "df_filtered['ingredients_per_step'] = df_filtered['n_ingredients'] / df_filtered['n_steps']\n",
    "# Extract nutrition information (example: calories as proxy for complexity)\n",
    "df_filtered['calories'] = df_filtered['nutrition'].apply(lambda x: eval(x)[0])\n",
    "\n",
    "# Select features\n",
    "original_features = ['n_ingredients', 'n_steps']\n",
    "engineered_features = ['ingredients_per_step', 'calories']\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale_original', StandardScaler(), original_features),\n",
    "        ('quantile_engineered', QuantileTransformer(n_quantiles=1000), engineered_features)\n",
    "    ])\n",
    "\n",
    "# Create pipeline\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', numeric_transformer),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100],\n",
    "    'regressor__max_depth': [5, 10],  # Reduced max_depth to prevent overfitting\n",
    "    'regressor__min_samples_split': [10]  # Increased to reduce overfitting\n",
    "}\n",
    "\n",
    "# Prepare data\n",
    "X = df_filtered[original_features + engineered_features]\n",
    "y = df_filtered['minutes']\n",
    "\n",
    "# Use same train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    final_pipeline,\n",
    "    param_grid,\n",
    "    cv=2,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model and evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
    "print(f\"\\nTest RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test R² Score: {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "importances = pd.DataFrame({\n",
    "    'feature': original_features + engineered_features,\n",
    "    'importance': best_model.named_steps['regressor'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "### Step 8: Fairness Analysis\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.express as px\n",
    "\n",
    "\"\"\"\n",
    "Fairness Question: Does our model perform worse for complex recipes compared to simple ones?\n",
    "\n",
    "Groups:\n",
    "- Simple recipes: <= 8 ingredients (median)\n",
    "- Complex recipes: > 8 ingredients\n",
    "\n",
    "Evaluation Metric: RMSE (appropriate for regression)\n",
    "\n",
    "Hypotheses:\n",
    "- Null: The model is fair; any difference in RMSE between simple and complex recipes is due to chance\n",
    "- Alternative: The model is unfair; RMSE for complex recipes is significantly different\n",
    "\"\"\"\n",
    "\n",
    "# Get predictions from our final model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Create groups based on number of ingredients (using median as threshold)\n",
    "median_ingredients = X_test['n_ingredients'].median()\n",
    "simple_mask = X_test['n_ingredients'] <= median_ingredients\n",
    "complex_mask = ~simple_mask\n",
    "\n",
    "# Calculate RMSE for each group\n",
    "simple_rmse = np.sqrt(mean_squared_error(y_test[simple_mask], y_pred[simple_mask]))\n",
    "complex_rmse = np.sqrt(mean_squared_error(y_test[complex_mask], y_pred[complex_mask]))\n",
    "\n",
    "# Calculate observed difference in RMSE\n",
    "observed_diff = complex_rmse - simple_rmse\n",
    "\n",
    "# Permutation test\n",
    "n_permutations = 1000\n",
    "diff_array = np.zeros(n_permutations)\n",
    "\n",
    "for i in range(n_permutations):\n",
    "    # Permute the group labels\n",
    "    permuted_mask = np.random.permutation(simple_mask)\n",
    "    \n",
    "    # Calculate RMSE for permuted groups\n",
    "    permuted_simple_rmse = np.sqrt(mean_squared_error(\n",
    "        y_test[permuted_mask], \n",
    "        y_pred[permuted_mask]\n",
    "    ))\n",
    "    permuted_complex_rmse = np.sqrt(mean_squared_error(\n",
    "        y_test[~permuted_mask], \n",
    "        y_pred[~permuted_mask]\n",
    "    ))\n",
    "    \n",
    "    # Store difference\n",
    "    diff_array[i] = permuted_complex_rmse - permuted_simple_rmse\n",
    "\n",
    "# Calculate p-value (two-sided test)\n",
    "p_value = np.mean(np.abs(diff_array) >= np.abs(observed_diff))\n",
    "\n",
    "print(\"\\nFairness Analysis Results:\")\n",
    "print(f\"RMSE for simple recipes: {simple_rmse:.4f}\")\n",
    "print(f\"RMSE for complex recipes: {complex_rmse:.4f}\")\n",
    "print(f\"Observed difference: {observed_diff:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Create visualization\n",
    "fig = px.histogram(\n",
    "    diff_array, \n",
    "    title='Permutation Test Results: Difference in RMSE between Complex and Simple Recipes',\n",
    "    labels={'value': 'Difference in RMSE', 'count': 'Frequency'}\n",
    ")\n",
    "fig.add_vline(x=observed_diff, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Observed Difference\")\n",
    "fig.add_vline(x=-observed_diff, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.show()\n",
    "\n",
    "# Save plot\n",
    "fig.write_html('assets/fairness_test.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fairness Analysis\n",
    "\n",
    "To assess the fairness of our final model, we examined whether the model performs significantly differently for recipes categorized as **simple** (≤8 ingredients) versus **complex** (>8 ingredients). The fairness question we sought to address was: **Does our model perform worse for complex recipes compared to simple recipes?** \n",
    "\n",
    "#### Groups Analyzed\n",
    "- **Group X (Simple Recipes)**: Recipes with 8 or fewer ingredients, representing simpler recipes.\n",
    "- **Group Y (Complex Recipes)**: Recipes with more than 8 ingredients, representing more intricate and challenging recipes.\n",
    "\n",
    "#### Evaluation Metric\n",
    "We used **Root Mean Squared Error (RMSE)** as our evaluation metric. RMSE is particularly suited for regression problems as it measures the average magnitude of prediction errors in the same units as the target variable (minutes). A lower RMSE indicates better performance.\n",
    "\n",
    "#### Hypotheses\n",
    "- **Null Hypothesis**: The model is fair; any observed difference in RMSE between simple and complex recipes is due to random chance.  \n",
    "- **Alternative Hypothesis**: The model is unfair; RMSE for complex recipes is significantly different from RMSE for simple recipes.\n",
    "\n",
    "#### Results\n",
    "The fairness analysis revealed the following metrics:\n",
    "- **RMSE for Simple Recipes**: 84.11 minutes\n",
    "- **RMSE for Complex Recipes**: 95.61 minutes\n",
    "- **Observed Difference in RMSE**: 11.50 minutes\n",
    "- **P-value**: 0.0000  \n",
    "\n",
    "The p-value was obtained using a permutation test with 1,000 iterations. By randomly shuffling the group labels and recalculating the RMSE difference for each permutation, we generated a distribution of RMSE differences under the null hypothesis. The observed difference (11.50 minutes) lies far outside this distribution, resulting in a p-value of approximately 0. This indicates that the observed difference is highly unlikely to have occurred due to random chance.\n",
    "\n",
    "#### Interpretation\n",
    "The extremely low p-value leads us to reject the null hypothesis in favor of the alternative hypothesis. This means that the model performs significantly worse for **complex recipes** compared to **simple recipes**. The higher RMSE for complex recipes suggests that the model struggles to capture the nuances and intricacies associated with more complex recipes, which often involve more ingredients and potentially non-linear interactions.\n",
    "\n",
    "#### Importance of Findings\n",
    "These results highlight a clear disparity in model performance between simple and complex recipes, which has practical implications:\n",
    "1. **Model Limitations**: The findings suggest that the model is biased toward simpler recipes and may not generalize well to more complex ones. This could result from insufficient feature engineering or the model's inability to effectively capture complex relationships within the data.\n",
    "2. **User Impact**: For users relying on the model to estimate preparation times for complex recipes, the increased error could lead to frustration or inaccurate expectations. This diminishes the model's usability and fairness for diverse recipe types.\n",
    "3. **Future Improvements**: To address this bias, future iterations of the model should incorporate more advanced features or techniques, such as natural language processing to extract richer information from recipe descriptions and steps, or ensemble methods to better handle non-linear interactions.\n",
    "\n",
    "#### Visualization\n",
    "The histogram below illustrates the distribution of RMSE differences generated through the permutation test, with the observed difference (11.50 minutes) marked by vertical red lines. The observed difference lies far outside the distribution, reinforcing the conclusion of significant disparity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
